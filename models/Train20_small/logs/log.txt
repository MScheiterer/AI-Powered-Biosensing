INFO 2025-05-05 18:15:31,837 train_utils.py: 108: MACHINE SEED: 6150
INFO 2025-05-05 18:15:31,844 train_utils.py: 154: Logging ENV_VARIABLES
INFO 2025-05-05 18:15:31,844 train_utils.py: 155: ALLUSERSPROFILE=C:\ProgramData
AMDRMPATH=C:\Program Files\AMD\RyzenMaster\
APPDATA=C:\Users\Micha\AppData\Roaming
CHROME_CRASHPAD_PIPE_NAME=\\.\pipe\crashpad_21796_LSNEPUQTWAQBQFZA
CLICOLOR=1
CLICOLOR_FORCE=1
COMMONPROGRAMFILES=C:\Program Files\Common Files
COMMONPROGRAMFILES(X86)=C:\Program Files (x86)\Common Files
COMMONPROGRAMW6432=C:\Program Files\Common Files
COMPUTERNAME=DESKTOP-SJKAOIV
COMSPEC=C:\WINDOWS\system32\cmd.exe
CUDA_MODULE_LOADING=LAZY
CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9
CUDA_PATH_V12_9=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9
DRIVERDATA=C:\Windows\System32\Drivers\DriverData
EFC_9016_1262719628=1
EFC_9016_1592913036=1
EFC_9016_2775293581=1
EFC_9016_3789132940=1
ELECTRON_RUN_AS_NODE=1
FORCE_COLOR=1
FPS_BROWSER_APP_PROFILE_STRING=Internet Explorer
FPS_BROWSER_USER_PROFILE_STRING=Default
GIT_PAGER=cat
HOMEDRIVE=C:
HOMEPATH=\Users\Micha
HYDRA_FULL_ERROR=1
JPY_INTERRUPT_EVENT=2560
LOCALAPPDATA=C:\Users\Micha\AppData\Local
LOCAL_RANK=0
LOGONSERVER=\\DESKTOP-SJKAOIV
MASTER_ADDR=localhost
MASTER_PORT=36957
MPLBACKEND=module://matplotlib_inline.backend_inline
NUMBER_OF_PROCESSORS=12
ONEDRIVE=C:\Users\Micha\OneDrive
ORIGINAL_XDG_CURRENT_DESKTOP=undefined
OS=Windows_NT
PAGER=cat
PATH=c:\Users\Micha\Desktop\BachelorProject\AI-Powered-Biosensing\.venv\Scripts;C:\Users\Micha\Desktop\BachelorProject\AI-Powered-Biosensing\.venv\Scripts;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\libnvvp;C:\mingw64\bin;C:\jdk-18.0.2\bin;C:\Program Files (x86)\Razer Chroma SDK\bin;C:\Program Files\Razer Chroma SDK\bin;C:\Program Files (x86)\Razer\ChromaBroadcast\bin;C:\Program Files\Razer\ChromaBroadcast\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\WINDOWS\system32\config\systemprofile\AppData\Local\Microsoft\WindowsApps;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;C:\apache-maven-3.9.6\bin;C:\Program Files\NVIDIA Corporation\NVIDIA app\NvDLISR;C:\Users\Micha\.local\bin;C:\Program Files\NVIDIA Corporation\Nsight Compute 2025.2.0\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Users\Micha\.local\bin;C:\Program Files\MySQL\MySQL Shell 8.0\bin\;C:\Users\Micha\AppData\Local\Microsoft\WindowsApps;C:\Microsoft VS Code\bin;C:\Gradle\gradle-8.4\bin;
PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC
PROCESSOR_ARCHITECTURE=AMD64
PROCESSOR_IDENTIFIER=AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
PROCESSOR_LEVEL=23
PROCESSOR_REVISION=7100
PROGRAMDATA=C:\ProgramData
PROGRAMFILES=C:\Program Files
PROGRAMFILES(X86)=C:\Program Files (x86)
PROGRAMW6432=C:\Program Files
PROMPT=(ai-powered-biosensing) $P$G
PSMODULEPATH=C:\Program Files\WindowsPowerShell\Modules;C:\WINDOWS\system32\WindowsPowerShell\v1.0\Modules
PUBLIC=C:\Users\Public
PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING=1
PYDEVD_USE_FRAME_EVAL=NO
PYTHONIOENCODING=utf-8
PYTHONUNBUFFERED=1
PYTHONUSERBASE=C:\Users\Micha\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages
PYTHON_FROZEN_MODULES=on
RANK=0
SESSIONNAME=Console
SYSTEMDRIVE=C:
SYSTEMROOT=C:\WINDOWS
TEMP=C:\Users\Micha\AppData\Local\Temp
TERM=xterm-color
TMP=C:\Users\Micha\AppData\Local\Temp
TORCH_NCCL_ASYNC_ERROR_HANDLING=1
USERDOMAIN=DESKTOP-SJKAOIV
USERDOMAIN_ROAMINGPROFILE=DESKTOP-SJKAOIV
USERNAME=Micha
USERPROFILE=C:\Users\Micha
VBOX_MSI_INSTALL_PATH=C:\Program Files\Oracle\VirtualBox\
VIRTUAL_ENV=C:\Users\Micha\Desktop\BachelorProject\AI-Powered-Biosensing\.venv
VIRTUAL_ENV_PROMPT=ai-powered-biosensing
VSCODE_CODE_CACHE_PATH=C:\Users\Micha\AppData\Roaming\Code\CachedData\17baf841131aa23349f217ca7c570c76ee87b957
VSCODE_CRASH_REPORTER_PROCESS_TYPE=extensionHost
VSCODE_CWD=C:\Microsoft VS Code
VSCODE_ESM_ENTRYPOINT=vs/workbench/api/node/extensionHostProcess
VSCODE_HANDLES_UNCAUGHT_ERRORS=true
VSCODE_IPC_HOOK=\\.\pipe\24d3f695-1.99.3-main-sock
VSCODE_L10N_BUNDLE_LOCATION=
VSCODE_NLS_CONFIG={"userLocale":"en-us","osLocale":"en-de","resolvedLanguage":"en","defaultMessagesFile":"C:\\Microsoft VS Code\\resources\\app\\out\\nls.messages.json","locale":"en-us","availableLanguages":{}}
VSCODE_PID=21796
WINDIR=C:\WINDOWS
WORLD_SIZE=1
_OLD_VIRTUAL_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\libnvvp;C:\mingw64\bin;C:\jdk-18.0.2\bin;C:\Program Files (x86)\Razer Chroma SDK\bin;C:\Program Files\Razer Chroma SDK\bin;C:\Program Files (x86)\Razer\ChromaBroadcast\bin;C:\Program Files\Razer\ChromaBroadcast\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\WINDOWS\system32\config\systemprofile\AppData\Local\Microsoft\WindowsApps;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;C:\apache-maven-3.9.6\bin;C:\Program Files\NVIDIA Corporation\NVIDIA app\NvDLISR;C:\Users\Micha\.local\bin;C:\Program Files\NVIDIA Corporation\Nsight Compute 2025.2.0\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Users\Micha\.local\bin;C:\Program Files\MySQL\MySQL Shell 8.0\bin\;C:\Users\Micha\AppData\Local\Microsoft\WindowsApps;C:\Microsoft VS Code\bin;C:\Gradle\gradle-8.4\bin;
_OLD_VIRTUAL_PROMPT=$P$G
__PSLOCKDOWNPOLICY=0

INFO 2025-05-05 18:15:31,845 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.
INFO 2025-05-05 18:15:31,846 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: C:/Users/Micha/Desktop/BachelorProject/AI-Powered-Biosensing/models/Train20_small/tensorboard
INFO 2025-05-05 18:15:32,433 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5
INFO 2025-05-05 18:15:32,436 trainer.py:1059: ====================
INFO 2025-05-05 18:15:32,436 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>
INFO 2025-05-05 18:15:32,439 trainer.py:1061: Model is SAM2Train(
  (image_encoder): ImageEncoder(
    (trunk): Hiera(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=96, out_features=384, bias=True)
              (1): Linear(in_features=384, out_features=96, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=192, out_features=768, bias=True)
              (1): Linear(in_features=768, out_features=192, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=192, out_features=768, bias=True)
              (1): Linear(in_features=768, out_features=192, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): Linear(in_features=1536, out_features=384, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
        )
        (4-13): 10 x MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): Linear(in_features=1536, out_features=384, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): Linear(in_features=3072, out_features=768, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): Linear(in_features=3072, out_features=768, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
      )
    )
    (neck): FpnNeck(
      (position_encoding): PositionEmbeddingSine()
      (convs): ModuleList(
        (0): Sequential(
          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Sequential(
          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Sequential(
          (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Sequential(
          (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))
  (memory_attention): MemoryAttention(
    (layers): ModuleList(
      (0-3): 4 x MemoryAttentionLayer(
        (self_attn): RoPEAttention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn_image): RoPEAttention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=64, out_features=256, bias=True)
          (v_proj): Linear(in_features=64, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (memory_encoder): MemoryEncoder(
    (mask_downsampler): MaskDownSampler(
      (encoder): Sequential(
        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): LayerNorm2d()
        (2): GELU(approximate='none')
        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): LayerNorm2d()
        (5): GELU(approximate='none')
        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (7): LayerNorm2d()
        (8): GELU(approximate='none')
        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (10): LayerNorm2d()
        (11): GELU(approximate='none')
        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fuser): Fuser(
      (proj): Identity()
      (layers): ModuleList(
        (0-1): 2 x CXBlock(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm2d()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
      )
    )
    (position_encoding): PositionEmbeddingSine()
    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (sam_prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (sam_mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): Linear(in_features=2048, out_features=256, bias=True)
            )
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (obj_score_token): Embedding(1, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
        (act): ReLU()
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
      (act): ReLU()
    )
    (pred_obj_score_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=1, bias=True)
      )
      (act): ReLU()
    )
  )
  (obj_ptr_proj): MLP(
    (layers): ModuleList(
      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
    )
    (act): ReLU()
  )
  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)
)
INFO 2025-05-05 18:15:32,440 trainer.py:1062: 	Total parameters 46.1 M
INFO 2025-05-05 18:15:32,440 trainer.py:1063: 	Trainable parameters 46.1 M
INFO 2025-05-05 18:15:32,440 trainer.py:1066: 	Non-Trainable parameters 0  
INFO 2025-05-05 18:15:32,440 trainer.py:1069: ====================
INFO 2025-05-05 18:15:32,444 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.
INFO 2025-05-05 18:15:32,444 trainer.py: 314: Moving components to device cuda:0 and local rank 0.
INFO 2025-05-05 18:15:32,519 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.
INFO 2025-05-05 18:15:32,542 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.3.proj.weight', 'image_encoder.trunk.blocks.1.proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.14.proj.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.14.proj.bias', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.1.proj.weight', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.3.proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias'}
INFO 2025-05-05 18:15:32,544 optimizer.py: 248: Matches for param_name [*bias*]: {'image_encoder.trunk.blocks.2.attn.proj.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'memory_attention.layers.0.linear1.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'memory_attention.layers.0.norm2.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'memory_attention.layers.1.norm1.bias', 'obj_ptr_tpos_proj.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'image_encoder.neck.convs.3.conv.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'obj_ptr_proj.layers.1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'image_encoder.trunk.blocks.1.proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'memory_attention.layers.3.linear2.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.14.proj.bias', 'memory_attention.layers.1.linear1.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'mask_downsample.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'memory_attention.layers.2.linear2.bias', 'obj_ptr_proj.layers.2.bias', 'image_encoder.neck.convs.2.conv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'memory_attention.layers.0.norm1.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'sam_mask_decoder.conv_s0.bias', 'memory_attention.layers.0.linear2.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'memory_encoder.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'memory_attention.layers.1.linear2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'obj_ptr_proj.layers.0.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'memory_attention.layers.2.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'image_encoder.neck.convs.0.conv.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'memory_encoder.pix_feat_proj.bias', 'image_encoder.neck.convs.1.conv.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'image_encoder.trunk.blocks.3.proj.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'memory_attention.layers.3.linear1.bias', 'memory_attention.layers.1.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'memory_attention.layers.2.linear1.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'memory_attention.norm.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'memory_attention.layers.3.norm2.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'sam_mask_decoder.conv_s1.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias'}
INFO 2025-05-05 18:15:32,545 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'memory_attention.layers.3.norm3.weight', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'memory_attention.layers.0.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.1.norm1.weight', 'memory_attention.layers.3.norm2.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'memory_attention.layers.3.norm1.bias', 'memory_attention.layers.1.norm3.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'memory_attention.layers.3.norm1.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'memory_attention.layers.2.norm3.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'memory_attention.layers.0.norm3.weight', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'memory_attention.norm.weight', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.13.norm2.weight', 'memory_attention.layers.2.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.15.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'memory_attention.layers.0.norm1.weight', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.10.norm2.weight', 'memory_attention.layers.0.norm2.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'memory_attention.layers.2.norm2.weight', 'memory_attention.norm.bias', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'memory_attention.layers.1.norm1.weight', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.13.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'memory_attention.layers.1.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.bias'} 
INFO 2025-05-05 18:15:34,315 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]
INFO 2025-05-05 18:15:34,502 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': '../checkpoints/sam2.1_hiera_small.pt', 'ckpt_state_dict_keys': ['model']}}
INFO 2025-05-05 18:15:47,366 train_utils.py: 271: Train Epoch: [0][ 0/40] | Batch Time: 12.71 (12.71) | Data Time: 10.13 (10.13) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 8.91e+00 (8.91e+00)
INFO 2025-05-05 18:15:55,005 train_utils.py: 271: Train Epoch: [0][10/40] | Batch Time: 0.74 (1.85) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 2.20e+01 (1.78e+01)
INFO 2025-05-05 18:16:02,561 train_utils.py: 271: Train Epoch: [0][20/40] | Batch Time: 0.71 (1.33) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 9.53e+00 (1.38e+01)
INFO 2025-05-05 18:16:10,015 train_utils.py: 271: Train Epoch: [0][30/40] | Batch Time: 0.79 (1.14) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 8.17e+00 (1.22e+01)
INFO 2025-05-05 18:16:17,286 trainer.py: 950: Estimated time remaining: 00d 00h 34m
INFO 2025-05-05 18:16:17,286 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:16:17,287 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 11.43487502336502, 'Losses/train_all_loss_mask': 0.16296570544072891, 'Losses/train_all_loss_dice': 7.10100302696228, 'Losses/train_all_loss_iou': 1.0745411982294173, 'Losses/train_all_loss_class': 1.668729345791764e-05, 'Losses/train_all_core_loss': 11.43487502336502, 'Trainer/where': 0.0195, 'Trainer/epoch': 0, 'Trainer/steps_train': 40}
INFO 2025-05-05 18:16:29,866 train_utils.py: 271: Train Epoch: [1][ 0/40] | Batch Time: 11.43 (11.43) | Data Time: 10.33 (10.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 8.16e+00 (8.16e+00)
INFO 2025-05-05 18:16:37,128 train_utils.py: 271: Train Epoch: [1][10/40] | Batch Time: 0.73 (1.70) | Data Time: 0.00 (0.94) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 8.20e+00 (9.16e+00)
INFO 2025-05-05 18:16:44,370 train_utils.py: 271: Train Epoch: [1][20/40] | Batch Time: 0.71 (1.24) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 9.47e+00 (8.88e+00)
INFO 2025-05-05 18:16:51,407 train_utils.py: 271: Train Epoch: [1][30/40] | Batch Time: 0.70 (1.06) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 9.32e+00 (8.68e+00)
INFO 2025-05-05 18:16:58,669 trainer.py: 950: Estimated time remaining: 00d 00h 31m
INFO 2025-05-05 18:16:58,670 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:16:58,670 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 8.602711594104766, 'Losses/train_all_loss_mask': 0.043611505530861905, 'Losses/train_all_loss_dice': 7.28752875328064, 'Losses/train_all_loss_iou': 0.44294945255387574, 'Losses/train_all_loss_class': 3.2795709781430205e-06, 'Losses/train_all_core_loss': 8.602711594104766, 'Trainer/where': 0.0395, 'Trainer/epoch': 1, 'Trainer/steps_train': 80}
INFO 2025-05-05 18:17:10,871 train_utils.py: 271: Train Epoch: [2][ 0/40] | Batch Time: 11.18 (11.18) | Data Time: 10.18 (10.18) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 5.33e+00 (5.33e+00)
INFO 2025-05-05 18:17:17,984 train_utils.py: 271: Train Epoch: [2][10/40] | Batch Time: 0.75 (1.66) | Data Time: 0.00 (0.93) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 8.16e+00 (8.01e+00)
INFO 2025-05-05 18:17:25,068 train_utils.py: 271: Train Epoch: [2][20/40] | Batch Time: 0.72 (1.21) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 7.08e+00 (8.06e+00)
INFO 2025-05-05 18:17:32,182 train_utils.py: 271: Train Epoch: [2][30/40] | Batch Time: 0.73 (1.05) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 8.15e+00 (7.84e+00)
INFO 2025-05-05 18:17:39,371 trainer.py: 950: Estimated time remaining: 00d 00h 30m
INFO 2025-05-05 18:17:39,373 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:17:39,373 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.873050093650818, 'Losses/train_all_loss_mask': 0.023185006431594955, 'Losses/train_all_loss_dice': 7.229171502590179, 'Losses/train_all_loss_iou': 0.18016701513552108, 'Losses/train_all_loss_class': 1.1377399028766888e-05, 'Losses/train_all_core_loss': 7.873050093650818, 'Trainer/where': 0.059500000000000004, 'Trainer/epoch': 2, 'Trainer/steps_train': 120}
INFO 2025-05-05 18:17:51,596 train_utils.py: 271: Train Epoch: [3][ 0/40] | Batch Time: 11.20 (11.20) | Data Time: 10.17 (10.17) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 8.13e+00 (8.13e+00)
INFO 2025-05-05 18:17:58,714 train_utils.py: 271: Train Epoch: [3][10/40] | Batch Time: 0.71 (1.66) | Data Time: 0.00 (0.93) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 8.02e+00 (7.79e+00)
INFO 2025-05-05 18:18:05,869 train_utils.py: 271: Train Epoch: [3][20/40] | Batch Time: 0.71 (1.21) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 8.06e+00 (7.69e+00)
INFO 2025-05-05 18:18:12,956 train_utils.py: 271: Train Epoch: [3][30/40] | Batch Time: 0.70 (1.05) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 7.59e+00 (7.68e+00)
INFO 2025-05-05 18:18:20,144 trainer.py: 950: Estimated time remaining: 00d 00h 29m
INFO 2025-05-05 18:18:20,144 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:18:20,144 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.7204788565635685, 'Losses/train_all_loss_mask': 0.012629211572129862, 'Losses/train_all_loss_dice': 7.372592437267303, 'Losses/train_all_loss_iou': 0.09529116369667463, 'Losses/train_all_loss_class': 1.1029630862857687e-05, 'Losses/train_all_core_loss': 7.7204788565635685, 'Trainer/where': 0.0795, 'Trainer/epoch': 3, 'Trainer/steps_train': 160}
INFO 2025-05-05 18:18:33,227 train_utils.py: 271: Train Epoch: [4][ 0/40] | Batch Time: 11.22 (11.22) | Data Time: 10.16 (10.16) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 7.99e+00 (7.99e+00)
INFO 2025-05-05 18:18:40,319 train_utils.py: 271: Train Epoch: [4][10/40] | Batch Time: 0.73 (1.66) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 8.06e+00 (7.76e+00)
INFO 2025-05-05 18:18:47,362 train_utils.py: 271: Train Epoch: [4][20/40] | Batch Time: 0.70 (1.21) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 7.93e+00 (7.73e+00)
INFO 2025-05-05 18:18:54,622 train_utils.py: 271: Train Epoch: [4][30/40] | Batch Time: 0.78 (1.05) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 8.06e+00 (7.82e+00)
INFO 2025-05-05 18:19:02,029 trainer.py: 950: Estimated time remaining: 00d 00h 29m
INFO 2025-05-05 18:19:02,030 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:19:02,030 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.912554478645324, 'Losses/train_all_loss_mask': 0.015177139770821668, 'Losses/train_all_loss_dice': 7.590556383132935, 'Losses/train_all_loss_iou': 0.018446688175026794, 'Losses/train_all_loss_class': 8.640416976390952e-06, 'Losses/train_all_core_loss': 7.912554478645324, 'Trainer/where': 0.09949999999999999, 'Trainer/epoch': 4, 'Trainer/steps_train': 200}
INFO 2025-05-05 18:19:14,194 train_utils.py: 271: Train Epoch: [5][ 0/40] | Batch Time: 11.11 (11.11) | Data Time: 10.18 (10.18) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 8.15e+00 (8.15e+00)
INFO 2025-05-05 18:19:21,287 train_utils.py: 271: Train Epoch: [5][10/40] | Batch Time: 0.70 (1.65) | Data Time: 0.00 (0.93) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 5.38e+00 (7.27e+00)
INFO 2025-05-05 18:19:28,418 train_utils.py: 271: Train Epoch: [5][20/40] | Batch Time: 0.76 (1.21) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 9.20e+00 (7.65e+00)
INFO 2025-05-05 18:19:35,442 train_utils.py: 271: Train Epoch: [5][30/40] | Batch Time: 0.71 (1.04) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 7.99e+00 (7.71e+00)
INFO 2025-05-05 18:19:42,719 trainer.py: 950: Estimated time remaining: 00d 00h 28m
INFO 2025-05-05 18:19:42,719 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:19:42,719 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.6517690300941466, 'Losses/train_all_loss_mask': 0.019512590578960952, 'Losses/train_all_loss_dice': 7.217820972204208, 'Losses/train_all_loss_iou': 0.04369011626695283, 'Losses/train_all_loss_class': 6.143555345472506e-06, 'Losses/train_all_core_loss': 7.6517690300941466, 'Trainer/where': 0.1195, 'Trainer/epoch': 5, 'Trainer/steps_train': 240}
INFO 2025-05-05 18:19:55,325 train_utils.py: 271: Train Epoch: [6][ 0/40] | Batch Time: 11.42 (11.42) | Data Time: 10.37 (10.37) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 8.05e+00 (8.05e+00)
INFO 2025-05-05 18:20:01,688 train_utils.py: 271: Train Epoch: [6][10/40] | Batch Time: 0.64 (1.62) | Data Time: 0.00 (0.94) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 8.30e+00 (7.66e+00)
INFO 2025-05-05 18:20:08,223 train_utils.py: 271: Train Epoch: [6][20/40] | Batch Time: 0.67 (1.16) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 7.86e+00 (7.81e+00)
INFO 2025-05-05 18:20:15,148 train_utils.py: 271: Train Epoch: [6][30/40] | Batch Time: 0.65 (1.01) | Data Time: 0.00 (0.34) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 7.98e+00 (7.79e+00)
INFO 2025-05-05 18:20:21,738 trainer.py: 950: Estimated time remaining: 00d 00h 26m
INFO 2025-05-05 18:20:21,738 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:20:21,738 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.6289497971534725, 'Losses/train_all_loss_mask': 0.028232281596137908, 'Losses/train_all_loss_dice': 7.026366817951202, 'Losses/train_all_loss_iou': 0.037931934419611936, 'Losses/train_all_loss_class': 5.423239335122787e-06, 'Losses/train_all_core_loss': 7.6289497971534725, 'Trainer/where': 0.13949999999999999, 'Trainer/epoch': 6, 'Trainer/steps_train': 280}
INFO 2025-05-05 18:20:34,428 train_utils.py: 271: Train Epoch: [7][ 0/40] | Batch Time: 11.62 (11.62) | Data Time: 10.64 (10.64) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 8.13e+00 (8.13e+00)
INFO 2025-05-05 18:20:41,322 train_utils.py: 271: Train Epoch: [7][10/40] | Batch Time: 0.67 (1.68) | Data Time: 0.00 (0.97) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 7.28e+00 (7.55e+00)
INFO 2025-05-05 18:20:48,269 train_utils.py: 271: Train Epoch: [7][20/40] | Batch Time: 0.69 (1.21) | Data Time: 0.00 (0.51) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 8.00e+00 (7.49e+00)
INFO 2025-05-05 18:20:55,149 train_utils.py: 271: Train Epoch: [7][30/40] | Batch Time: 0.68 (1.04) | Data Time: 0.00 (0.34) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 8.00e+00 (7.56e+00)
INFO 2025-05-05 18:21:01,651 trainer.py: 950: Estimated time remaining: 00d 00h 26m
INFO 2025-05-05 18:21:01,652 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:21:01,652 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.623284941911697, 'Losses/train_all_loss_mask': 0.01464938365315902, 'Losses/train_all_loss_dice': 7.314521390199661, 'Losses/train_all_loss_iou': 0.015771369252615842, 'Losses/train_all_loss_class': 4.568028166573867e-06, 'Losses/train_all_core_loss': 7.623284941911697, 'Trainer/where': 0.1595, 'Trainer/epoch': 7, 'Trainer/steps_train': 320}
INFO 2025-05-05 18:21:13,555 train_utils.py: 271: Train Epoch: [8][ 0/40] | Batch Time: 10.93 (10.93) | Data Time: 9.95 (9.95) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.10e+01 (1.10e+01)
INFO 2025-05-05 18:21:19,994 train_utils.py: 271: Train Epoch: [8][10/40] | Batch Time: 0.62 (1.58) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 7.76e+00 (7.85e+00)
INFO 2025-05-05 18:21:26,191 train_utils.py: 271: Train Epoch: [8][20/40] | Batch Time: 0.62 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 6.69e+00 (7.69e+00)
INFO 2025-05-05 18:21:32,560 train_utils.py: 271: Train Epoch: [8][30/40] | Batch Time: 0.63 (0.97) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 8.09e+00 (7.54e+00)
INFO 2025-05-05 18:21:38,901 trainer.py: 950: Estimated time remaining: 00d 00h 24m
INFO 2025-05-05 18:21:38,902 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:21:38,902 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.494626796245575, 'Losses/train_all_loss_mask': 0.01308943017538695, 'Losses/train_all_loss_dice': 7.21575368642807, 'Losses/train_all_loss_iou': 0.01707760391291231, 'Losses/train_all_loss_class': 6.969078283702856e-06, 'Losses/train_all_core_loss': 7.494626796245575, 'Trainer/where': 0.1795, 'Trainer/epoch': 8, 'Trainer/steps_train': 360}
INFO 2025-05-05 18:21:50,668 train_utils.py: 271: Train Epoch: [9][ 0/40] | Batch Time: 10.79 (10.79) | Data Time: 9.94 (9.94) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 8.10e+00 (8.10e+00)
INFO 2025-05-05 18:21:56,930 train_utils.py: 271: Train Epoch: [9][10/40] | Batch Time: 0.64 (1.55) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 7.70e+00 (7.75e+00)
INFO 2025-05-05 18:22:03,223 train_utils.py: 271: Train Epoch: [9][20/40] | Batch Time: 0.62 (1.11) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 5.37e+00 (7.59e+00)
INFO 2025-05-05 18:22:09,428 train_utils.py: 271: Train Epoch: [9][30/40] | Batch Time: 0.61 (0.95) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 7.81e+00 (7.51e+00)
INFO 2025-05-05 18:22:15,770 trainer.py: 950: Estimated time remaining: 00d 00h 23m
INFO 2025-05-05 18:22:15,771 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:22:15,771 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.545971858501434, 'Losses/train_all_loss_mask': 0.01686312615929637, 'Losses/train_all_loss_dice': 7.1541402518749235, 'Losses/train_all_loss_iou': 0.054564078024122865, 'Losses/train_all_loss_class': 5.100863800944211e-06, 'Losses/train_all_core_loss': 7.545971858501434, 'Trainer/where': 0.19949999999999998, 'Trainer/epoch': 9, 'Trainer/steps_train': 400}
INFO 2025-05-05 18:22:27,657 train_utils.py: 271: Train Epoch: [10][ 0/40] | Batch Time: 10.86 (10.86) | Data Time: 9.95 (9.95) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 7.96e+00 (7.96e+00)
INFO 2025-05-05 18:22:34,094 train_utils.py: 271: Train Epoch: [10][10/40] | Batch Time: 0.63 (1.57) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 8.33e+00 (7.83e+00)
INFO 2025-05-05 18:22:40,325 train_utils.py: 271: Train Epoch: [10][20/40] | Batch Time: 0.61 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 8.62e+00 (7.81e+00)
INFO 2025-05-05 18:22:46,639 train_utils.py: 271: Train Epoch: [10][30/40] | Batch Time: 0.64 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 7.46e+00 (7.63e+00)
INFO 2025-05-05 18:22:53,009 trainer.py: 950: Estimated time remaining: 00d 00h 23m
INFO 2025-05-05 18:22:53,009 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:22:53,010 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.6171722412109375, 'Losses/train_all_loss_mask': 0.011209629968652735, 'Losses/train_all_loss_dice': 7.382851457595825, 'Losses/train_all_loss_iou': 0.01012344561968348, 'Losses/train_all_loss_class': 4.782472209363675e-06, 'Losses/train_all_core_loss': 7.6171722412109375, 'Trainer/where': 0.2195, 'Trainer/epoch': 10, 'Trainer/steps_train': 440}
INFO 2025-05-05 18:23:04,899 train_utils.py: 271: Train Epoch: [11][ 0/40] | Batch Time: 10.93 (10.93) | Data Time: 9.99 (9.99) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 7.90e+00 (7.90e+00)
INFO 2025-05-05 18:23:11,229 train_utils.py: 271: Train Epoch: [11][10/40] | Batch Time: 0.60 (1.57) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 7.95e+00 (8.15e+00)
INFO 2025-05-05 18:23:17,542 train_utils.py: 271: Train Epoch: [11][20/40] | Batch Time: 0.60 (1.12) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 8.19e+00 (7.97e+00)
INFO 2025-05-05 18:23:23,782 train_utils.py: 271: Train Epoch: [11][30/40] | Batch Time: 0.63 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 7.78e+00 (7.78e+00)
INFO 2025-05-05 18:23:30,223 trainer.py: 950: Estimated time remaining: 00d 00h 22m
INFO 2025-05-05 18:23:30,224 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:23:30,224 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.793500816822052, 'Losses/train_all_loss_mask': 0.01493116648925934, 'Losses/train_all_loss_dice': 7.480881381034851, 'Losses/train_all_loss_iou': 0.013990036798713846, 'Losses/train_all_loss_class': 6.1669756505011716e-06, 'Losses/train_all_core_loss': 7.793500816822052, 'Trainer/where': 0.2395, 'Trainer/epoch': 11, 'Trainer/steps_train': 480}
INFO 2025-05-05 18:23:41,985 train_utils.py: 271: Train Epoch: [12][ 0/40] | Batch Time: 10.79 (10.79) | Data Time: 9.96 (9.96) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 7.95e+00 (7.95e+00)
INFO 2025-05-05 18:23:48,376 train_utils.py: 271: Train Epoch: [12][10/40] | Batch Time: 0.63 (1.56) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 7.32e+00 (6.82e+00)
INFO 2025-05-05 18:23:54,790 train_utils.py: 271: Train Epoch: [12][20/40] | Batch Time: 0.64 (1.12) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 7.68e+00 (7.18e+00)
INFO 2025-05-05 18:24:01,259 train_utils.py: 271: Train Epoch: [12][30/40] | Batch Time: 0.66 (0.97) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 7.61e+00 (7.35e+00)
INFO 2025-05-05 18:24:07,721 trainer.py: 950: Estimated time remaining: 00d 00h 22m
INFO 2025-05-05 18:24:07,721 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:24:07,721 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.485544288158417, 'Losses/train_all_loss_mask': 0.012304694643171387, 'Losses/train_all_loss_dice': 7.203140938282013, 'Losses/train_all_loss_iou': 0.03630505019791599, 'Losses/train_all_loss_class': 4.38903136696922e-06, 'Losses/train_all_core_loss': 7.485544288158417, 'Trainer/where': 0.2595, 'Trainer/epoch': 12, 'Trainer/steps_train': 520}
INFO 2025-05-05 18:24:19,671 train_utils.py: 271: Train Epoch: [13][ 0/40] | Batch Time: 10.98 (10.98) | Data Time: 9.94 (9.94) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 5.19e+00 (5.19e+00)
INFO 2025-05-05 18:24:26,015 train_utils.py: 271: Train Epoch: [13][10/40] | Batch Time: 0.64 (1.58) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 7.97e+00 (7.50e+00)
INFO 2025-05-05 18:24:32,340 train_utils.py: 271: Train Epoch: [13][20/40] | Batch Time: 0.64 (1.13) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 8.12e+00 (7.40e+00)
INFO 2025-05-05 18:24:38,636 train_utils.py: 271: Train Epoch: [13][30/40] | Batch Time: 0.63 (0.97) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 5.16e+00 (7.27e+00)
INFO 2025-05-05 18:24:45,414 trainer.py: 950: Estimated time remaining: 00d 00h 21m
INFO 2025-05-05 18:24:45,414 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:24:45,415 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.338911628723144, 'Losses/train_all_loss_mask': 0.014478437723300885, 'Losses/train_all_loss_dice': 6.994773483276367, 'Losses/train_all_loss_iou': 0.054566338717631876, 'Losses/train_all_loss_class': 3.0082790245522517e-06, 'Losses/train_all_core_loss': 7.338911628723144, 'Trainer/where': 0.27949999999999997, 'Trainer/epoch': 13, 'Trainer/steps_train': 560}
INFO 2025-05-05 18:24:57,815 train_utils.py: 271: Train Epoch: [14][ 0/40] | Batch Time: 10.93 (10.93) | Data Time: 9.95 (9.95) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 8.00e+00 (8.00e+00)
INFO 2025-05-05 18:25:04,178 train_utils.py: 271: Train Epoch: [14][10/40] | Batch Time: 0.67 (1.57) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 7.78e+00 (7.49e+00)
INFO 2025-05-05 18:25:10,432 train_utils.py: 271: Train Epoch: [14][20/40] | Batch Time: 0.61 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 7.83e+00 (7.59e+00)
INFO 2025-05-05 18:25:16,837 train_utils.py: 271: Train Epoch: [14][30/40] | Batch Time: 0.64 (0.97) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 8.08e+00 (7.53e+00)
INFO 2025-05-05 18:25:23,427 trainer.py: 950: Estimated time remaining: 00d 00h 20m
INFO 2025-05-05 18:25:23,427 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:25:23,427 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.573029279708862, 'Losses/train_all_loss_mask': 0.011480230653251055, 'Losses/train_all_loss_dice': 7.326436674594879, 'Losses/train_all_loss_iou': 0.016986027663733694, 'Losses/train_all_loss_class': 1.963433783602753e-06, 'Losses/train_all_core_loss': 7.573029279708862, 'Trainer/where': 0.2995, 'Trainer/epoch': 14, 'Trainer/steps_train': 600}
INFO 2025-05-05 18:25:35,490 train_utils.py: 271: Train Epoch: [15][ 0/40] | Batch Time: 11.06 (11.06) | Data Time: 10.07 (10.07) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 7.80e+00 (7.80e+00)
INFO 2025-05-05 18:25:42,301 train_utils.py: 271: Train Epoch: [15][10/40] | Batch Time: 0.68 (1.63) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 5.20e+00 (7.43e+00)
INFO 2025-05-05 18:25:49,065 train_utils.py: 271: Train Epoch: [15][20/40] | Batch Time: 0.68 (1.17) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 8.42e+00 (7.22e+00)
INFO 2025-05-05 18:25:55,724 train_utils.py: 271: Train Epoch: [15][30/40] | Batch Time: 0.66 (1.01) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 7.98e+00 (7.28e+00)
INFO 2025-05-05 18:26:02,423 trainer.py: 950: Estimated time remaining: 00d 00h 21m
INFO 2025-05-05 18:26:02,423 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:26:02,423 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.360460996627808, 'Losses/train_all_loss_mask': 0.010824295000929851, 'Losses/train_all_loss_dice': 7.125809776782989, 'Losses/train_all_loss_iou': 0.018162787614710397, 'Losses/train_all_loss_class': 2.582993123390054e-06, 'Losses/train_all_core_loss': 7.360460996627808, 'Trainer/where': 0.3195, 'Trainer/epoch': 15, 'Trainer/steps_train': 640}
INFO 2025-05-05 18:26:14,634 train_utils.py: 271: Train Epoch: [16][ 0/40] | Batch Time: 11.19 (11.19) | Data Time: 10.25 (10.25) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 7.95e+00 (7.95e+00)
INFO 2025-05-05 18:26:22,106 train_utils.py: 271: Train Epoch: [16][10/40] | Batch Time: 0.71 (1.70) | Data Time: 0.00 (0.93) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 7.93e+00 (7.33e+00)
INFO 2025-05-05 18:26:28,930 train_utils.py: 271: Train Epoch: [16][20/40] | Batch Time: 0.64 (1.21) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 7.97e+00 (7.34e+00)
INFO 2025-05-05 18:26:35,563 train_utils.py: 271: Train Epoch: [16][30/40] | Batch Time: 0.65 (1.04) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 7.73e+00 (7.19e+00)
INFO 2025-05-05 18:26:42,221 trainer.py: 950: Estimated time remaining: 00d 00h 20m
INFO 2025-05-05 18:26:42,222 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:26:42,222 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.275007784366608, 'Losses/train_all_loss_mask': 0.012416510137336445, 'Losses/train_all_loss_dice': 7.0180357694625854, 'Losses/train_all_loss_iou': 0.0086376594594185, 'Losses/train_all_loss_class': 4.159484020149762e-06, 'Losses/train_all_core_loss': 7.275007784366608, 'Trainer/where': 0.3395, 'Trainer/epoch': 16, 'Trainer/steps_train': 680}
INFO 2025-05-05 18:26:54,222 train_utils.py: 271: Train Epoch: [17][ 0/40] | Batch Time: 11.03 (11.03) | Data Time: 10.02 (10.02) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 7.93e+00 (7.93e+00)
INFO 2025-05-05 18:27:00,809 train_utils.py: 271: Train Epoch: [17][10/40] | Batch Time: 0.66 (1.60) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 7.70e+00 (7.40e+00)
INFO 2025-05-05 18:27:07,384 train_utils.py: 271: Train Epoch: [17][20/40] | Batch Time: 0.65 (1.15) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 7.79e+00 (7.29e+00)
INFO 2025-05-05 18:27:13,980 train_utils.py: 271: Train Epoch: [17][30/40] | Batch Time: 0.64 (0.99) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 8.25e+00 (7.31e+00)
INFO 2025-05-05 18:27:20,655 trainer.py: 950: Estimated time remaining: 00d 00h 19m
INFO 2025-05-05 18:27:20,656 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:27:20,656 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.456274783611297, 'Losses/train_all_loss_mask': 0.016413928324982407, 'Losses/train_all_loss_dice': 7.103285920619965, 'Losses/train_all_loss_iou': 0.0247069959397777, 'Losses/train_all_loss_class': 3.3513168133048055e-06, 'Losses/train_all_core_loss': 7.456274783611297, 'Trainer/where': 0.35950000000000004, 'Trainer/epoch': 17, 'Trainer/steps_train': 720}
INFO 2025-05-05 18:27:33,581 train_utils.py: 271: Train Epoch: [18][ 0/40] | Batch Time: 11.06 (11.06) | Data Time: 10.14 (10.14) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 5.16e+00 (5.16e+00)
INFO 2025-05-05 18:27:40,174 train_utils.py: 271: Train Epoch: [18][10/40] | Batch Time: 0.63 (1.60) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 7.95e+00 (6.75e+00)
INFO 2025-05-05 18:27:46,458 train_utils.py: 271: Train Epoch: [18][20/40] | Batch Time: 0.62 (1.14) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 7.95e+00 (7.23e+00)
INFO 2025-05-05 18:27:52,777 train_utils.py: 271: Train Epoch: [18][30/40] | Batch Time: 0.63 (0.98) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 7.68e+00 (7.21e+00)
INFO 2025-05-05 18:27:59,576 trainer.py: 950: Estimated time remaining: 00d 00h 18m
INFO 2025-05-05 18:27:59,577 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:27:59,577 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.342315918207168, 'Losses/train_all_loss_mask': 0.009927743862499483, 'Losses/train_all_loss_dice': 7.1226288139820095, 'Losses/train_all_loss_iou': 0.021122397844374065, 'Losses/train_all_loss_class': 9.813487906340512e-06, 'Losses/train_all_core_loss': 7.342315918207168, 'Trainer/where': 0.3795, 'Trainer/epoch': 18, 'Trainer/steps_train': 760}
INFO 2025-05-05 18:28:12,819 train_utils.py: 271: Train Epoch: [19][ 0/40] | Batch Time: 11.32 (11.32) | Data Time: 10.23 (10.23) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 8.62e+00 (8.62e+00)
INFO 2025-05-05 18:28:19,874 train_utils.py: 271: Train Epoch: [19][10/40] | Batch Time: 0.68 (1.67) | Data Time: 0.00 (0.93) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 5.33e+00 (7.42e+00)
INFO 2025-05-05 18:28:26,838 train_utils.py: 271: Train Epoch: [19][20/40] | Batch Time: 0.70 (1.21) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 8.62e+00 (7.47e+00)
INFO 2025-05-05 18:28:33,691 train_utils.py: 271: Train Epoch: [19][30/40] | Batch Time: 0.68 (1.04) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 7.84e+00 (7.43e+00)
INFO 2025-05-05 18:28:40,722 trainer.py: 950: Estimated time remaining: 00d 00h 19m
INFO 2025-05-05 18:28:40,723 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:28:40,723 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.4130069494247435, 'Losses/train_all_loss_mask': 0.012072092348535079, 'Losses/train_all_loss_dice': 7.129309451580047, 'Losses/train_all_loss_iou': 0.042254263711947716, 'Losses/train_all_loss_class': 1.4500013569618274e-06, 'Losses/train_all_core_loss': 7.4130069494247435, 'Trainer/where': 0.3995, 'Trainer/epoch': 19, 'Trainer/steps_train': 800}
INFO 2025-05-05 18:28:52,854 train_utils.py: 271: Train Epoch: [20][ 0/40] | Batch Time: 11.07 (11.07) | Data Time: 9.98 (9.98) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 7.97e+00 (7.97e+00)
INFO 2025-05-05 18:28:59,943 train_utils.py: 271: Train Epoch: [20][10/40] | Batch Time: 0.70 (1.65) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 7.91e+00 (7.19e+00)
INFO 2025-05-05 18:29:06,920 train_utils.py: 271: Train Epoch: [20][20/40] | Batch Time: 0.70 (1.20) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 7.66e+00 (7.25e+00)
INFO 2025-05-05 18:29:13,915 train_utils.py: 271: Train Epoch: [20][30/40] | Batch Time: 0.68 (1.04) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 7.98e+00 (7.20e+00)
INFO 2025-05-05 18:29:20,862 trainer.py: 950: Estimated time remaining: 00d 00h 18m
INFO 2025-05-05 18:29:20,862 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:29:20,864 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.214847588539124, 'Losses/train_all_loss_mask': 0.010890733480482594, 'Losses/train_all_loss_dice': 6.9668325066566466, 'Losses/train_all_loss_iou': 0.030197153705739766, 'Losses/train_all_loss_class': 3.2699292903615174e-06, 'Losses/train_all_core_loss': 7.214847588539124, 'Trainer/where': 0.41950000000000004, 'Trainer/epoch': 20, 'Trainer/steps_train': 840}
INFO 2025-05-05 18:29:33,262 train_utils.py: 271: Train Epoch: [21][ 0/40] | Batch Time: 11.40 (11.40) | Data Time: 10.44 (10.44) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 6.29e+00 (6.29e+00)
INFO 2025-05-05 18:29:40,434 train_utils.py: 271: Train Epoch: [21][10/40] | Batch Time: 0.69 (1.69) | Data Time: 0.00 (0.95) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 7.95e+00 (7.19e+00)
INFO 2025-05-05 18:29:46,674 train_utils.py: 271: Train Epoch: [21][20/40] | Batch Time: 0.65 (1.18) | Data Time: 0.00 (0.50) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 7.67e+00 (7.09e+00)
INFO 2025-05-05 18:29:52,887 train_utils.py: 271: Train Epoch: [21][30/40] | Batch Time: 0.62 (1.00) | Data Time: 0.00 (0.34) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 7.90e+00 (7.35e+00)
INFO 2025-05-05 18:29:59,232 trainer.py: 950: Estimated time remaining: 00d 00h 17m
INFO 2025-05-05 18:29:59,233 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:29:59,233 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.370232009887696, 'Losses/train_all_loss_mask': 0.009627295647078427, 'Losses/train_all_loss_dice': 7.147249680757523, 'Losses/train_all_loss_iou': 0.030435059833689593, 'Losses/train_all_loss_class': 1.3938351609121824e-06, 'Losses/train_all_core_loss': 7.370232009887696, 'Trainer/where': 0.4395, 'Trainer/epoch': 21, 'Trainer/steps_train': 880}
INFO 2025-05-05 18:30:11,027 train_utils.py: 271: Train Epoch: [22][ 0/40] | Batch Time: 10.82 (10.82) | Data Time: 9.79 (9.79) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 7.75e+00 (7.75e+00)
INFO 2025-05-05 18:30:17,497 train_utils.py: 271: Train Epoch: [22][10/40] | Batch Time: 0.66 (1.57) | Data Time: 0.00 (0.89) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 5.37e+00 (7.09e+00)
INFO 2025-05-05 18:30:24,193 train_utils.py: 271: Train Epoch: [22][20/40] | Batch Time: 0.69 (1.14) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 7.84e+00 (7.41e+00)
INFO 2025-05-05 18:30:30,847 train_utils.py: 271: Train Epoch: [22][30/40] | Batch Time: 0.68 (0.99) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 7.90e+00 (7.44e+00)
INFO 2025-05-05 18:30:38,163 trainer.py: 950: Estimated time remaining: 00d 00h 16m
INFO 2025-05-05 18:30:38,164 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:30:38,164 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.481361770629883, 'Losses/train_all_loss_mask': 0.014386302095954307, 'Losses/train_all_loss_dice': 7.141909456253051, 'Losses/train_all_loss_iou': 0.05172107804010011, 'Losses/train_all_loss_class': 5.237977990413256e-06, 'Losses/train_all_core_loss': 7.481361770629883, 'Trainer/where': 0.4595, 'Trainer/epoch': 22, 'Trainer/steps_train': 920}
INFO 2025-05-05 18:30:50,300 train_utils.py: 271: Train Epoch: [23][ 0/40] | Batch Time: 11.09 (11.09) | Data Time: 10.08 (10.08) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 7.95e+00 (7.95e+00)
INFO 2025-05-05 18:30:56,872 train_utils.py: 271: Train Epoch: [23][10/40] | Batch Time: 0.65 (1.61) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 5.27e+00 (7.26e+00)
INFO 2025-05-05 18:31:03,095 train_utils.py: 271: Train Epoch: [23][20/40] | Batch Time: 0.60 (1.14) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 8.00e+00 (7.46e+00)
INFO 2025-05-05 18:31:09,344 train_utils.py: 271: Train Epoch: [23][30/40] | Batch Time: 0.62 (0.97) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 7.68e+00 (7.37e+00)
INFO 2025-05-05 18:31:15,751 trainer.py: 950: Estimated time remaining: 00d 00h 15m
INFO 2025-05-05 18:31:15,751 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:31:15,751 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.185362052917481, 'Losses/train_all_loss_mask': 0.011189725435542641, 'Losses/train_all_loss_dice': 6.934968429803848, 'Losses/train_all_loss_iou': 0.026597191815926634, 'Losses/train_all_loss_class': 1.8961997251265572e-06, 'Losses/train_all_core_loss': 7.185362052917481, 'Trainer/where': 0.47950000000000004, 'Trainer/epoch': 23, 'Trainer/steps_train': 960}
INFO 2025-05-05 18:31:27,444 train_utils.py: 271: Train Epoch: [24][ 0/40] | Batch Time: 10.75 (10.75) | Data Time: 9.86 (9.86) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 7.81e+00 (7.81e+00)
INFO 2025-05-05 18:31:33,929 train_utils.py: 271: Train Epoch: [24][10/40] | Batch Time: 0.65 (1.57) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 8.23e+00 (7.59e+00)
INFO 2025-05-05 18:31:40,649 train_utils.py: 271: Train Epoch: [24][20/40] | Batch Time: 0.73 (1.14) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 7.33e+00 (7.20e+00)
INFO 2025-05-05 18:31:47,195 train_utils.py: 271: Train Epoch: [24][30/40] | Batch Time: 0.61 (0.98) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 8.15e+00 (7.09e+00)
INFO 2025-05-05 18:31:53,930 trainer.py: 950: Estimated time remaining: 00d 00h 15m
INFO 2025-05-05 18:31:53,931 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:31:53,931 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.203789114952087, 'Losses/train_all_loss_mask': 0.011141017837508116, 'Losses/train_all_loss_dice': 6.941827297210693, 'Losses/train_all_loss_iou': 0.03914056254543539, 'Losses/train_all_loss_class': 9.874843628576714e-07, 'Losses/train_all_core_loss': 7.203789114952087, 'Trainer/where': 0.49950000000000006, 'Trainer/epoch': 24, 'Trainer/steps_train': 1000}
INFO 2025-05-05 18:32:05,951 train_utils.py: 271: Train Epoch: [25][ 0/40] | Batch Time: 11.06 (11.06) | Data Time: 10.14 (10.14) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 7.59e+00 (7.59e+00)
INFO 2025-05-05 18:32:12,616 train_utils.py: 271: Train Epoch: [25][10/40] | Batch Time: 0.64 (1.61) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 8.36e+00 (7.27e+00)
INFO 2025-05-05 18:32:19,164 train_utils.py: 271: Train Epoch: [25][20/40] | Batch Time: 0.70 (1.16) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 7.69e+00 (7.39e+00)
INFO 2025-05-05 18:32:25,625 train_utils.py: 271: Train Epoch: [25][30/40] | Batch Time: 0.65 (0.99) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 7.95e+00 (7.38e+00)
INFO 2025-05-05 18:32:32,273 trainer.py: 950: Estimated time remaining: 00d 00h 14m
INFO 2025-05-05 18:32:32,274 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:32:32,274 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.444756853580475, 'Losses/train_all_loss_mask': 0.009115921013290063, 'Losses/train_all_loss_dice': 7.216242337226868, 'Losses/train_all_loss_iou': 0.04619541406209464, 'Losses/train_all_loss_class': 7.531578670283424e-07, 'Losses/train_all_core_loss': 7.444756853580475, 'Trainer/where': 0.5195000000000001, 'Trainer/epoch': 25, 'Trainer/steps_train': 1040}
INFO 2025-05-05 18:32:44,968 train_utils.py: 271: Train Epoch: [26][ 0/40] | Batch Time: 11.21 (11.21) | Data Time: 10.17 (10.17) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 7.65e+00 (7.65e+00)
INFO 2025-05-05 18:32:51,476 train_utils.py: 271: Train Epoch: [26][10/40] | Batch Time: 0.63 (1.61) | Data Time: 0.00 (0.93) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 9.66e+00 (7.93e+00)
INFO 2025-05-05 18:32:57,870 train_utils.py: 271: Train Epoch: [26][20/40] | Batch Time: 0.61 (1.15) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 7.97e+00 (7.72e+00)
INFO 2025-05-05 18:33:04,307 train_utils.py: 271: Train Epoch: [26][30/40] | Batch Time: 0.64 (0.99) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 4.45e+00 (7.54e+00)
INFO 2025-05-05 18:33:10,804 trainer.py: 950: Estimated time remaining: 00d 00h 13m
INFO 2025-05-05 18:33:10,805 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:33:10,805 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.452136957645417, 'Losses/train_all_loss_mask': 0.01669345124028041, 'Losses/train_all_loss_dice': 7.058617824316025, 'Losses/train_all_loss_iou': 0.059649373977299545, 'Losses/train_all_loss_class': 8.00095268660428e-07, 'Losses/train_all_core_loss': 7.452136957645417, 'Trainer/where': 0.5395, 'Trainer/epoch': 26, 'Trainer/steps_train': 1080}
INFO 2025-05-05 18:33:23,641 train_utils.py: 271: Train Epoch: [27][ 0/40] | Batch Time: 11.16 (11.16) | Data Time: 10.31 (10.31) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 4.09e+00 (4.09e+00)
INFO 2025-05-05 18:33:30,281 train_utils.py: 271: Train Epoch: [27][10/40] | Batch Time: 0.64 (1.62) | Data Time: 0.00 (0.94) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 6.58e+00 (6.08e+00)
INFO 2025-05-05 18:33:36,842 train_utils.py: 271: Train Epoch: [27][20/40] | Batch Time: 0.66 (1.16) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 7.21e+00 (6.36e+00)
INFO 2025-05-05 18:33:43,580 train_utils.py: 271: Train Epoch: [27][30/40] | Batch Time: 0.66 (1.00) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 7.81e+00 (6.74e+00)
INFO 2025-05-05 18:33:50,292 trainer.py: 950: Estimated time remaining: 00d 00h 13m
INFO 2025-05-05 18:33:50,292 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:33:50,292 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 6.842869889736176, 'Losses/train_all_loss_mask': 0.009524805967521388, 'Losses/train_all_loss_dice': 6.609342557191849, 'Losses/train_all_loss_iou': 0.04302325391481645, 'Losses/train_all_loss_class': 7.95073365571497e-06, 'Losses/train_all_core_loss': 6.842869889736176, 'Trainer/where': 0.5595, 'Trainer/epoch': 27, 'Trainer/steps_train': 1120}
INFO 2025-05-05 18:34:02,920 train_utils.py: 271: Train Epoch: [28][ 0/40] | Batch Time: 11.08 (11.08) | Data Time: 10.16 (10.16) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 7.97e+00 (7.97e+00)
INFO 2025-05-05 18:34:09,544 train_utils.py: 271: Train Epoch: [28][10/40] | Batch Time: 0.67 (1.61) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 4.93e+00 (7.01e+00)
INFO 2025-05-05 18:34:16,012 train_utils.py: 271: Train Epoch: [28][20/40] | Batch Time: 0.64 (1.15) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 7.95e+00 (7.36e+00)
INFO 2025-05-05 18:34:22,611 train_utils.py: 271: Train Epoch: [28][30/40] | Batch Time: 0.65 (0.99) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 7.66e+00 (7.48e+00)
INFO 2025-05-05 18:34:29,278 trainer.py: 950: Estimated time remaining: 00d 00h 12m
INFO 2025-05-05 18:34:29,279 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:34:29,279 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.3039135336875916, 'Losses/train_all_loss_mask': 0.010582788370811613, 'Losses/train_all_loss_dice': 7.042002999782563, 'Losses/train_all_loss_iou': 0.050253436199272986, 'Losses/train_all_loss_class': 1.3880748161332334e-06, 'Losses/train_all_core_loss': 7.3039135336875916, 'Trainer/where': 0.5795, 'Trainer/epoch': 28, 'Trainer/steps_train': 1160}
INFO 2025-05-05 18:34:41,883 train_utils.py: 271: Train Epoch: [29][ 0/40] | Batch Time: 11.05 (11.05) | Data Time: 10.15 (10.15) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 7.70e+00 (7.70e+00)
INFO 2025-05-05 18:34:48,575 train_utils.py: 271: Train Epoch: [29][10/40] | Batch Time: 0.66 (1.61) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 7.83e+00 (7.43e+00)
INFO 2025-05-05 18:34:55,087 train_utils.py: 271: Train Epoch: [29][20/40] | Batch Time: 0.68 (1.15) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 7.84e+00 (7.46e+00)
INFO 2025-05-05 18:35:01,688 train_utils.py: 271: Train Epoch: [29][30/40] | Batch Time: 0.67 (1.00) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 7.08e+00 (7.49e+00)
INFO 2025-05-05 18:35:08,360 trainer.py: 950: Estimated time remaining: 00d 00h 12m
INFO 2025-05-05 18:35:08,360 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:35:08,361 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.482531321048737, 'Losses/train_all_loss_mask': 0.012314827966474696, 'Losses/train_all_loss_dice': 7.189734160900116, 'Losses/train_all_loss_iou': 0.04649975437823741, 'Losses/train_all_loss_class': 8.161867089739872e-07, 'Losses/train_all_core_loss': 7.482531321048737, 'Trainer/where': 0.5995, 'Trainer/epoch': 29, 'Trainer/steps_train': 1200}
INFO 2025-05-05 18:35:20,262 train_utils.py: 271: Train Epoch: [30][ 0/40] | Batch Time: 10.93 (10.93) | Data Time: 10.02 (10.02) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 7.81e+00 (7.81e+00)
INFO 2025-05-05 18:35:26,969 train_utils.py: 271: Train Epoch: [30][10/40] | Batch Time: 0.67 (1.60) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 7.29e+00 (7.12e+00)
INFO 2025-05-05 18:35:33,452 train_utils.py: 271: Train Epoch: [30][20/40] | Batch Time: 0.65 (1.15) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 7.84e+00 (7.56e+00)
INFO 2025-05-05 18:35:40,054 train_utils.py: 271: Train Epoch: [30][30/40] | Batch Time: 0.65 (0.99) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 8.00e+00 (7.34e+00)
INFO 2025-05-05 18:35:46,776 trainer.py: 950: Estimated time remaining: 00d 00h 11m
INFO 2025-05-05 18:35:46,777 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:35:46,777 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.283784592151642, 'Losses/train_all_loss_mask': 0.010142810738034313, 'Losses/train_all_loss_dice': 7.065735125541687, 'Losses/train_all_loss_iou': 0.015189659653333364, 'Losses/train_all_loss_class': 3.6668919908056096e-06, 'Losses/train_all_core_loss': 7.283784592151642, 'Trainer/where': 0.6195, 'Trainer/epoch': 30, 'Trainer/steps_train': 1240}
INFO 2025-05-05 18:35:58,775 train_utils.py: 271: Train Epoch: [31][ 0/40] | Batch Time: 11.02 (11.02) | Data Time: 10.03 (10.03) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 7.51e+00 (7.51e+00)
INFO 2025-05-05 18:36:05,492 train_utils.py: 271: Train Epoch: [31][10/40] | Batch Time: 0.64 (1.61) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 7.93e+00 (7.74e+00)
INFO 2025-05-05 18:36:12,041 train_utils.py: 271: Train Epoch: [31][20/40] | Batch Time: 0.68 (1.16) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 7.45e+00 (7.69e+00)
INFO 2025-05-05 18:36:18,744 train_utils.py: 271: Train Epoch: [31][30/40] | Batch Time: 0.68 (1.00) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 7.55e+00 (7.30e+00)
INFO 2025-05-05 18:36:25,372 trainer.py: 950: Estimated time remaining: 00d 00h 11m
INFO 2025-05-05 18:36:25,374 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:36:25,374 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.381524586677552, 'Losses/train_all_loss_mask': 0.011930641523213126, 'Losses/train_all_loss_dice': 7.112397855520248, 'Losses/train_all_loss_iou': 0.03051306485049281, 'Losses/train_all_loss_class': 9.597774906833222e-07, 'Losses/train_all_core_loss': 7.381524586677552, 'Trainer/where': 0.6395000000000001, 'Trainer/epoch': 31, 'Trainer/steps_train': 1280}
INFO 2025-05-05 18:36:37,638 train_utils.py: 271: Train Epoch: [32][ 0/40] | Batch Time: 11.10 (11.10) | Data Time: 10.06 (10.06) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 5.25e+00 (5.25e+00)
INFO 2025-05-05 18:36:44,296 train_utils.py: 271: Train Epoch: [32][10/40] | Batch Time: 0.65 (1.61) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 6.87e+00 (6.80e+00)
INFO 2025-05-05 18:36:50,798 train_utils.py: 271: Train Epoch: [32][20/40] | Batch Time: 0.64 (1.16) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 8.04e+00 (7.13e+00)
INFO 2025-05-05 18:36:57,439 train_utils.py: 271: Train Epoch: [32][30/40] | Batch Time: 0.66 (1.00) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 7.89e+00 (7.11e+00)
INFO 2025-05-05 18:37:04,140 trainer.py: 950: Estimated time remaining: 00d 00h 10m
INFO 2025-05-05 18:37:04,141 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:37:04,141 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.121925628185272, 'Losses/train_all_loss_mask': 0.009509370691739604, 'Losses/train_all_loss_dice': 6.897854721546173, 'Losses/train_all_loss_iou': 0.03388101581385854, 'Losses/train_all_loss_class': 2.5452388839330986e-06, 'Losses/train_all_core_loss': 7.121925628185272, 'Trainer/where': 0.6595, 'Trainer/epoch': 32, 'Trainer/steps_train': 1320}
INFO 2025-05-05 18:37:16,170 train_utils.py: 271: Train Epoch: [33][ 0/40] | Batch Time: 11.06 (11.06) | Data Time: 10.19 (10.19) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 7.94e+00 (7.94e+00)
INFO 2025-05-05 18:37:22,804 train_utils.py: 271: Train Epoch: [33][10/40] | Batch Time: 0.66 (1.61) | Data Time: 0.00 (0.93) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 7.70e+00 (7.34e+00)
INFO 2025-05-05 18:37:29,422 train_utils.py: 271: Train Epoch: [33][20/40] | Batch Time: 0.66 (1.16) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 5.28e+00 (7.20e+00)
INFO 2025-05-05 18:37:35,950 train_utils.py: 271: Train Epoch: [33][30/40] | Batch Time: 0.66 (0.99) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 7.85e+00 (7.46e+00)
INFO 2025-05-05 18:37:42,789 trainer.py: 950: Estimated time remaining: 00d 00h 09m
INFO 2025-05-05 18:37:42,789 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:37:42,789 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.4528455376625065, 'Losses/train_all_loss_mask': 0.010591152011329541, 'Losses/train_all_loss_dice': 7.222283148765564, 'Losses/train_all_loss_iou': 0.018738403985844344, 'Losses/train_all_loss_class': 9.491361717461899e-07, 'Losses/train_all_core_loss': 7.4528455376625065, 'Trainer/where': 0.6795, 'Trainer/epoch': 33, 'Trainer/steps_train': 1360}
INFO 2025-05-05 18:37:54,851 train_utils.py: 271: Train Epoch: [34][ 0/40] | Batch Time: 11.08 (11.08) | Data Time: 10.09 (10.09) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 6.87e+00 (6.87e+00)
INFO 2025-05-05 18:38:01,615 train_utils.py: 271: Train Epoch: [34][10/40] | Batch Time: 0.66 (1.62) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 8.10e+00 (7.58e+00)
INFO 2025-05-05 18:38:08,192 train_utils.py: 271: Train Epoch: [34][20/40] | Batch Time: 0.66 (1.16) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 7.75e+00 (7.68e+00)
INFO 2025-05-05 18:38:14,826 train_utils.py: 271: Train Epoch: [34][30/40] | Batch Time: 0.65 (1.00) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 5.04e+00 (7.51e+00)
INFO 2025-05-05 18:38:21,514 trainer.py: 950: Estimated time remaining: 00d 00h 09m
INFO 2025-05-05 18:38:21,514 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:38:21,514 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.5132187604904175, 'Losses/train_all_loss_mask': 0.015474177216674434, 'Losses/train_all_loss_dice': 7.156760716438294, 'Losses/train_all_loss_iou': 0.046973868076929645, 'Losses/train_all_loss_class': 6.188070966750558e-07, 'Losses/train_all_core_loss': 7.5132187604904175, 'Trainer/where': 0.6995, 'Trainer/epoch': 34, 'Trainer/steps_train': 1400}
INFO 2025-05-05 18:38:33,990 train_utils.py: 271: Train Epoch: [35][ 0/40] | Batch Time: 11.19 (11.19) | Data Time: 10.12 (10.12) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 8.91e+00 (8.91e+00)
INFO 2025-05-05 18:38:40,583 train_utils.py: 271: Train Epoch: [35][10/40] | Batch Time: 0.65 (1.62) | Data Time: 0.00 (0.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 7.44e+00 (7.47e+00)
INFO 2025-05-05 18:38:47,155 train_utils.py: 271: Train Epoch: [35][20/40] | Batch Time: 0.65 (1.16) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 7.32e+00 (7.09e+00)
INFO 2025-05-05 18:38:53,744 train_utils.py: 271: Train Epoch: [35][30/40] | Batch Time: 0.65 (1.00) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 7.63e+00 (7.17e+00)
INFO 2025-05-05 18:39:00,465 trainer.py: 950: Estimated time remaining: 00d 00h 08m
INFO 2025-05-05 18:39:00,466 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:39:00,466 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.172591030597687, 'Losses/train_all_loss_mask': 0.010349903639871627, 'Losses/train_all_loss_dice': 6.921386957168579, 'Losses/train_all_loss_iou': 0.04420523952740041, 'Losses/train_all_loss_class': 7.765991210106904e-07, 'Losses/train_all_core_loss': 7.172591030597687, 'Trainer/where': 0.7195, 'Trainer/epoch': 35, 'Trainer/steps_train': 1440}
INFO 2025-05-05 18:39:12,802 train_utils.py: 271: Train Epoch: [36][ 0/40] | Batch Time: 11.35 (11.35) | Data Time: 10.29 (10.29) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 7.80e+00 (7.80e+00)
INFO 2025-05-05 18:39:19,828 train_utils.py: 271: Train Epoch: [36][10/40] | Batch Time: 0.68 (1.67) | Data Time: 0.00 (0.94) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 8.14e+00 (7.73e+00)
INFO 2025-05-05 18:39:26,140 train_utils.py: 271: Train Epoch: [36][20/40] | Batch Time: 0.67 (1.18) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 7.77e+00 (7.84e+00)
INFO 2025-05-05 18:39:32,450 train_utils.py: 271: Train Epoch: [36][30/40] | Batch Time: 0.62 (1.00) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 7.94e+00 (7.61e+00)
INFO 2025-05-05 18:39:38,858 trainer.py: 950: Estimated time remaining: 00d 00h 07m
INFO 2025-05-05 18:39:38,858 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:39:38,858 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.521298241615296, 'Losses/train_all_loss_mask': 0.0188844253665593, 'Losses/train_all_loss_dice': 7.1180167436599735, 'Losses/train_all_loss_iou': 0.025592207461340877, 'Losses/train_all_loss_class': 8.679953567236609e-07, 'Losses/train_all_core_loss': 7.521298241615296, 'Trainer/where': 0.7395, 'Trainer/epoch': 36, 'Trainer/steps_train': 1480}
INFO 2025-05-05 18:39:50,728 train_utils.py: 271: Train Epoch: [37][ 0/40] | Batch Time: 10.90 (10.90) | Data Time: 9.93 (9.93) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 6.70e+00 (6.70e+00)
INFO 2025-05-05 18:39:57,103 train_utils.py: 271: Train Epoch: [37][10/40] | Batch Time: 0.66 (1.57) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 3.00e+00 (7.25e+00)
INFO 2025-05-05 18:40:03,453 train_utils.py: 271: Train Epoch: [37][20/40] | Batch Time: 0.63 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 7.43e+00 (7.49e+00)
INFO 2025-05-05 18:40:09,802 train_utils.py: 271: Train Epoch: [37][30/40] | Batch Time: 0.63 (0.97) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 7.86e+00 (7.43e+00)
INFO 2025-05-05 18:40:16,286 trainer.py: 950: Estimated time remaining: 00d 00h 07m
INFO 2025-05-05 18:40:16,286 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:40:16,287 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.367186635732651, 'Losses/train_all_loss_mask': 0.009426396647540969, 'Losses/train_all_loss_dice': 7.125805824995041, 'Losses/train_all_loss_iou': 0.05285233029644587, 'Losses/train_all_loss_class': 5.798529292055932e-07, 'Losses/train_all_core_loss': 7.367186635732651, 'Trainer/where': 0.7595000000000001, 'Trainer/epoch': 37, 'Trainer/steps_train': 1520}
INFO 2025-05-05 18:40:28,932 train_utils.py: 271: Train Epoch: [38][ 0/40] | Batch Time: 10.89 (10.89) | Data Time: 9.91 (9.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 8.37e+00 (8.37e+00)
INFO 2025-05-05 18:40:35,333 train_utils.py: 271: Train Epoch: [38][10/40] | Batch Time: 0.62 (1.57) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 7.76e+00 (6.91e+00)
INFO 2025-05-05 18:40:41,612 train_utils.py: 271: Train Epoch: [38][20/40] | Batch Time: 0.62 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 4.43e+00 (7.04e+00)
INFO 2025-05-05 18:40:48,008 train_utils.py: 271: Train Epoch: [38][30/40] | Batch Time: 0.66 (0.97) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 7.93e+00 (7.18e+00)
INFO 2025-05-05 18:40:54,413 trainer.py: 950: Estimated time remaining: 00d 00h 06m
INFO 2025-05-05 18:40:54,414 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:40:54,414 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.335263359546661, 'Losses/train_all_loss_mask': 0.010732802716302104, 'Losses/train_all_loss_dice': 7.089712101221084, 'Losses/train_all_loss_iou': 0.03089325674227439, 'Losses/train_all_loss_class': 1.9537988550588637e-06, 'Losses/train_all_core_loss': 7.335263359546661, 'Trainer/where': 0.7795000000000001, 'Trainer/epoch': 38, 'Trainer/steps_train': 1560}
INFO 2025-05-05 18:41:06,284 train_utils.py: 271: Train Epoch: [39][ 0/40] | Batch Time: 10.77 (10.77) | Data Time: 9.92 (9.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 7.79e+00 (7.79e+00)
INFO 2025-05-05 18:41:12,700 train_utils.py: 271: Train Epoch: [39][10/40] | Batch Time: 0.63 (1.56) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 7.97e+00 (7.06e+00)
INFO 2025-05-05 18:41:18,996 train_utils.py: 271: Train Epoch: [39][20/40] | Batch Time: 0.62 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 7.74e+00 (7.21e+00)
INFO 2025-05-05 18:41:25,231 train_utils.py: 271: Train Epoch: [39][30/40] | Batch Time: 0.64 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 9.43e+00 (7.47e+00)
INFO 2025-05-05 18:41:31,592 trainer.py: 950: Estimated time remaining: 00d 00h 05m
INFO 2025-05-05 18:41:31,593 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:41:31,593 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.441253709793091, 'Losses/train_all_loss_mask': 0.013411630326299929, 'Losses/train_all_loss_dice': 7.1329894185066225, 'Losses/train_all_loss_iou': 0.04002506210599677, 'Losses/train_all_loss_class': 6.647307390950097e-06, 'Losses/train_all_core_loss': 7.441253709793091, 'Trainer/where': 0.7995, 'Trainer/epoch': 39, 'Trainer/steps_train': 1600}
INFO 2025-05-05 18:41:43,330 train_utils.py: 271: Train Epoch: [40][ 0/40] | Batch Time: 10.77 (10.77) | Data Time: 9.88 (9.88) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 7.54e+00 (7.54e+00)
INFO 2025-05-05 18:41:49,697 train_utils.py: 271: Train Epoch: [40][10/40] | Batch Time: 0.66 (1.56) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 6.25e+00 (6.67e+00)
INFO 2025-05-05 18:41:56,052 train_utils.py: 271: Train Epoch: [40][20/40] | Batch Time: 0.62 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 7.90e+00 (7.06e+00)
INFO 2025-05-05 18:42:02,300 train_utils.py: 271: Train Epoch: [40][30/40] | Batch Time: 0.68 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 8.86e+00 (7.04e+00)
INFO 2025-05-05 18:42:08,727 trainer.py: 950: Estimated time remaining: 00d 00h 05m
INFO 2025-05-05 18:42:08,728 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:42:08,728 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.210147166252137, 'Losses/train_all_loss_mask': 0.010862416684358323, 'Losses/train_all_loss_dice': 6.963893330097198, 'Losses/train_all_loss_iou': 0.029003012647353898, 'Losses/train_all_loss_class': 2.5010124327318196e-06, 'Losses/train_all_core_loss': 7.210147166252137, 'Trainer/where': 0.8195, 'Trainer/epoch': 40, 'Trainer/steps_train': 1640}
INFO 2025-05-05 18:42:20,508 train_utils.py: 271: Train Epoch: [41][ 0/40] | Batch Time: 10.82 (10.82) | Data Time: 9.90 (9.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 8.00e+00 (8.00e+00)
INFO 2025-05-05 18:42:26,857 train_utils.py: 271: Train Epoch: [41][10/40] | Batch Time: 0.62 (1.56) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 7.99e+00 (6.88e+00)
INFO 2025-05-05 18:42:33,095 train_utils.py: 271: Train Epoch: [41][20/40] | Batch Time: 0.62 (1.11) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 8.47e+00 (7.18e+00)
INFO 2025-05-05 18:42:39,317 train_utils.py: 271: Train Epoch: [41][30/40] | Batch Time: 0.61 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 8.33e+00 (7.02e+00)
INFO 2025-05-05 18:42:45,733 trainer.py: 950: Estimated time remaining: 00d 00h 04m
INFO 2025-05-05 18:42:45,734 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:42:45,734 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.119647824764252, 'Losses/train_all_loss_mask': 0.010787420645465317, 'Losses/train_all_loss_dice': 6.879896581172943, 'Losses/train_all_loss_iou': 0.02399613051829874, 'Losses/train_all_loss_class': 6.715632072806788e-06, 'Losses/train_all_core_loss': 7.119647824764252, 'Trainer/where': 0.8395, 'Trainer/epoch': 41, 'Trainer/steps_train': 1680}
INFO 2025-05-05 18:42:57,835 train_utils.py: 271: Train Epoch: [42][ 0/40] | Batch Time: 10.70 (10.70) | Data Time: 9.87 (9.87) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 7.83e+00 (7.83e+00)
INFO 2025-05-05 18:43:04,210 train_utils.py: 271: Train Epoch: [42][10/40] | Batch Time: 0.63 (1.55) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 5.07e+00 (6.93e+00)
INFO 2025-05-05 18:43:10,475 train_utils.py: 271: Train Epoch: [42][20/40] | Batch Time: 0.63 (1.11) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 5.32e+00 (7.02e+00)
INFO 2025-05-05 18:43:16,898 train_utils.py: 271: Train Epoch: [42][30/40] | Batch Time: 0.65 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 7.96e+00 (6.97e+00)
INFO 2025-05-05 18:43:23,279 trainer.py: 950: Estimated time remaining: 00d 00h 04m
INFO 2025-05-05 18:43:23,279 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:43:23,279 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.106515169143677, 'Losses/train_all_loss_mask': 0.010646099928271724, 'Losses/train_all_loss_dice': 6.864306080341339, 'Losses/train_all_loss_iou': 0.029280073832796915, 'Losses/train_all_loss_class': 7.028025929423088e-06, 'Losses/train_all_core_loss': 7.106515169143677, 'Trainer/where': 0.8595, 'Trainer/epoch': 42, 'Trainer/steps_train': 1720}
INFO 2025-05-05 18:43:35,093 train_utils.py: 271: Train Epoch: [43][ 0/40] | Batch Time: 10.86 (10.86) | Data Time: 9.83 (9.83) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 6.67e+00 (6.67e+00)
INFO 2025-05-05 18:43:41,442 train_utils.py: 271: Train Epoch: [43][10/40] | Batch Time: 0.63 (1.56) | Data Time: 0.00 (0.89) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 7.53e+00 (7.10e+00)
INFO 2025-05-05 18:43:47,684 train_utils.py: 271: Train Epoch: [43][20/40] | Batch Time: 0.64 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 7.93e+00 (7.25e+00)
INFO 2025-05-05 18:43:53,956 train_utils.py: 271: Train Epoch: [43][30/40] | Batch Time: 0.61 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 7.01e+00 (7.34e+00)
INFO 2025-05-05 18:44:00,484 trainer.py: 950: Estimated time remaining: 00d 00h 03m
INFO 2025-05-05 18:44:00,485 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:44:00,485 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.445122838020325, 'Losses/train_all_loss_mask': 0.011877063308929792, 'Losses/train_all_loss_dice': 7.1779728591442105, 'Losses/train_all_loss_iou': 0.02960613564846426, 'Losses/train_all_loss_class': 2.650616612198675e-06, 'Losses/train_all_core_loss': 7.445122838020325, 'Trainer/where': 0.8795000000000001, 'Trainer/epoch': 43, 'Trainer/steps_train': 1760}
INFO 2025-05-05 18:44:12,807 train_utils.py: 271: Train Epoch: [44][ 0/40] | Batch Time: 11.16 (11.16) | Data Time: 10.29 (10.29) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 8.58e+00 (8.58e+00)
INFO 2025-05-05 18:44:19,129 train_utils.py: 271: Train Epoch: [44][10/40] | Batch Time: 0.65 (1.59) | Data Time: 0.00 (0.94) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 7.79e+00 (7.84e+00)
INFO 2025-05-05 18:44:25,524 train_utils.py: 271: Train Epoch: [44][20/40] | Batch Time: 0.64 (1.14) | Data Time: 0.00 (0.49) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 7.96e+00 (7.46e+00)
INFO 2025-05-05 18:44:31,805 train_utils.py: 271: Train Epoch: [44][30/40] | Batch Time: 0.62 (0.97) | Data Time: 0.00 (0.33) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 8.35e+00 (7.49e+00)
INFO 2025-05-05 18:44:38,370 trainer.py: 950: Estimated time remaining: 00d 00h 02m
INFO 2025-05-05 18:44:38,370 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:44:38,371 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.49586318731308, 'Losses/train_all_loss_mask': 0.009771720424032538, 'Losses/train_all_loss_dice': 7.274471533298493, 'Losses/train_all_loss_iou': 0.02595653028920424, 'Losses/train_all_loss_class': 7.460341082454747e-07, 'Losses/train_all_core_loss': 7.49586318731308, 'Trainer/where': 0.8995000000000001, 'Trainer/epoch': 44, 'Trainer/steps_train': 1800}
INFO 2025-05-05 18:44:50,225 train_utils.py: 271: Train Epoch: [45][ 0/40] | Batch Time: 10.88 (10.88) | Data Time: 10.00 (10.00) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 5.35e+00 (5.35e+00)
INFO 2025-05-05 18:44:56,588 train_utils.py: 271: Train Epoch: [45][10/40] | Batch Time: 0.63 (1.57) | Data Time: 0.00 (0.91) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 8.12e+00 (7.34e+00)
INFO 2025-05-05 18:45:02,885 train_utils.py: 271: Train Epoch: [45][20/40] | Batch Time: 0.64 (1.12) | Data Time: 0.00 (0.48) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 7.81e+00 (7.40e+00)
INFO 2025-05-05 18:45:09,098 train_utils.py: 271: Train Epoch: [45][30/40] | Batch Time: 0.62 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 4.61e+00 (7.25e+00)
INFO 2025-05-05 18:45:15,533 trainer.py: 950: Estimated time remaining: 00d 00h 02m
INFO 2025-05-05 18:45:15,534 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:45:15,534 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.256488436460495, 'Losses/train_all_loss_mask': 0.012405436688277404, 'Losses/train_all_loss_dice': 6.968784689903259, 'Losses/train_all_loss_iou': 0.03959339129314685, 'Losses/train_all_loss_class': 1.6070692246572537e-06, 'Losses/train_all_core_loss': 7.256488436460495, 'Trainer/where': 0.9195, 'Trainer/epoch': 45, 'Trainer/steps_train': 1840}
INFO 2025-05-05 18:45:27,617 train_utils.py: 271: Train Epoch: [46][ 0/40] | Batch Time: 10.92 (10.92) | Data Time: 9.95 (9.95) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 7.57e+00 (7.57e+00)
INFO 2025-05-05 18:45:33,936 train_utils.py: 271: Train Epoch: [46][10/40] | Batch Time: 0.62 (1.57) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 8.15e+00 (7.82e+00)
INFO 2025-05-05 18:45:40,217 train_utils.py: 271: Train Epoch: [46][20/40] | Batch Time: 0.64 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 8.05e+00 (7.80e+00)
INFO 2025-05-05 18:45:46,538 train_utils.py: 271: Train Epoch: [46][30/40] | Batch Time: 0.59 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 7.61e+00 (7.56e+00)
INFO 2025-05-05 18:45:53,091 trainer.py: 950: Estimated time remaining: 00d 00h 01m
INFO 2025-05-05 18:45:53,091 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:45:53,092 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.54784209728241, 'Losses/train_all_loss_mask': 0.012542244681390003, 'Losses/train_all_loss_dice': 7.231840115785599, 'Losses/train_all_loss_iou': 0.0651565534333713, 'Losses/train_all_loss_class': 5.927948657280524e-07, 'Losses/train_all_core_loss': 7.54784209728241, 'Trainer/where': 0.9395, 'Trainer/epoch': 46, 'Trainer/steps_train': 1880}
INFO 2025-05-05 18:46:04,798 train_utils.py: 271: Train Epoch: [47][ 0/40] | Batch Time: 10.72 (10.72) | Data Time: 9.88 (9.88) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 7.97e+00 (7.97e+00)
INFO 2025-05-05 18:46:11,119 train_utils.py: 271: Train Epoch: [47][10/40] | Batch Time: 0.62 (1.55) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 7.84e+00 (7.20e+00)
INFO 2025-05-05 18:46:17,452 train_utils.py: 271: Train Epoch: [47][20/40] | Batch Time: 0.65 (1.11) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 7.92e+00 (7.46e+00)
INFO 2025-05-05 18:46:23,849 train_utils.py: 271: Train Epoch: [47][30/40] | Batch Time: 0.65 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 7.95e+00 (7.46e+00)
INFO 2025-05-05 18:46:30,185 trainer.py: 950: Estimated time remaining: 00d 00h 01m
INFO 2025-05-05 18:46:30,186 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:46:30,186 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.4675565481185915, 'Losses/train_all_loss_mask': 0.012890486069227335, 'Losses/train_all_loss_dice': 7.163747429847717, 'Losses/train_all_loss_iou': 0.04599899363006443, 'Losses/train_all_loss_class': 4.360130964609077e-07, 'Losses/train_all_core_loss': 7.4675565481185915, 'Trainer/where': 0.9595, 'Trainer/epoch': 47, 'Trainer/steps_train': 1920}
INFO 2025-05-05 18:46:41,986 train_utils.py: 271: Train Epoch: [48][ 0/40] | Batch Time: 10.79 (10.79) | Data Time: 9.92 (9.92) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 7.94e+00 (7.94e+00)
INFO 2025-05-05 18:46:48,309 train_utils.py: 271: Train Epoch: [48][10/40] | Batch Time: 0.63 (1.56) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 4.87e+00 (6.94e+00)
INFO 2025-05-05 18:46:54,562 train_utils.py: 271: Train Epoch: [48][20/40] | Batch Time: 0.64 (1.11) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 7.66e+00 (7.20e+00)
INFO 2025-05-05 18:47:00,782 train_utils.py: 271: Train Epoch: [48][30/40] | Batch Time: 0.63 (0.95) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 7.99e+00 (7.17e+00)
INFO 2025-05-05 18:47:07,170 trainer.py: 950: Estimated time remaining: 00d 00h 00m
INFO 2025-05-05 18:47:07,171 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:47:07,171 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.233135831356049, 'Losses/train_all_loss_mask': 0.009631150560016977, 'Losses/train_all_loss_dice': 6.938403451442719, 'Losses/train_all_loss_iou': 0.10210815147997891, 'Losses/train_all_loss_class': 1.2194258271591707e-06, 'Losses/train_all_core_loss': 7.233135831356049, 'Trainer/where': 0.9795, 'Trainer/epoch': 48, 'Trainer/steps_train': 1960}
INFO 2025-05-05 18:47:18,969 train_utils.py: 271: Train Epoch: [49][ 0/40] | Batch Time: 10.82 (10.82) | Data Time: 9.87 (9.87) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 7.88e+00 (7.88e+00)
INFO 2025-05-05 18:47:25,354 train_utils.py: 271: Train Epoch: [49][10/40] | Batch Time: 0.62 (1.56) | Data Time: 0.00 (0.90) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 7.64e+00 (7.10e+00)
INFO 2025-05-05 18:47:31,642 train_utils.py: 271: Train Epoch: [49][20/40] | Batch Time: 0.66 (1.12) | Data Time: 0.00 (0.47) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 7.74e+00 (7.04e+00)
INFO 2025-05-05 18:47:37,883 train_utils.py: 271: Train Epoch: [49][30/40] | Batch Time: 0.63 (0.96) | Data Time: 0.00 (0.32) | Mem (GB): 5.00 (5.00/5.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 7.96e+00 (7.12e+00)
INFO 2025-05-05 18:47:44,405 trainer.py: 950: Estimated time remaining: 00d 00h 00m
INFO 2025-05-05 18:47:44,406 trainer.py: 892: Synchronizing meters
INFO 2025-05-05 18:47:44,406 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 7.233644443750381, 'Losses/train_all_loss_mask': 0.008966131001943722, 'Losses/train_all_loss_dice': 7.011635705828667, 'Losses/train_all_loss_iou': 0.04268575411733764, 'Losses/train_all_loss_class': 4.882583680698716e-07, 'Losses/train_all_core_loss': 7.233644443750381, 'Trainer/where': 0.9995, 'Trainer/epoch': 49, 'Trainer/steps_train': 2000}
